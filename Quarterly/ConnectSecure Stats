import requests
import pandas as pd
from typing import Dict, List, Tuple
import time
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Airtable configuration (Replace with your actual values)
AIRTABLE_API_KEY = ""
AIRTABLE_BASE_ID = ""
AIRTABLE_TABLE_NAME = ""

# ConnectSecure configuration
BASE_URL = "https://<Your Pod Here>.myconnectsecure.com"
CLIENT_AUTH_TOKEN = ""  # Replace with your actual token

# API Rate Limiting - Adjust as needed
REQUESTS_PER_MINUTE = 60  # Example: Limit to 60 requests per minute
SECONDS_PER_REQUEST = 60 / REQUESTS_PER_MINUTE

def get_auth_token(base_url: str, client_auth_token: str) -> Tuple[str, str]:
    # ... (This function remains the same) ...
    """
    Get authentication token from the API

    Args:
        base_url: Base URL for the API
        client_auth_token: Authentication token for the client

    Returns:
        Tuple containing access token and user ID

    Raises:
        requests.exceptions.RequestException: If API request fails
        ValueError: If response is missing required fields
    """
    try:
        headers = {
            'Client-Auth-Token': client_auth_token
        }

        response = requests.post(f"{base_url}/w/authorize", headers=headers, timeout=30)
        response.raise_for_status()

        data = response.json()
        if not data.get('access_token') or not data.get('user_id'):
            raise ValueError("Response missing required authentication fields")

        return data['access_token'], data['user_id']
    except requests.exceptions.RequestException as e:
        logging.error(f"Authentication request failed: {str(e)}")
        raise
    except ValueError as e:
        logging.error(f"Invalid authentication response: {str(e)}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during authentication: {str(e)}")
        raise

def get_companies(base_url: str, access_token: str, user_id: str) -> List[Dict]:
    # ... (This function remains the same) ...
    """
    Get list of all companies

    Args:
        base_url: Base URL for the API
        access_token: Valid access token
        user_id: User ID for authentication

    Returns:
        List of company dictionaries

    Raises:
        requests.exceptions.RequestException: If API request fails
        ValueError: If response data is invalid
    """
    try:
        headers = {
            'Authorization': f'Bearer {access_token}',
            'X-USER-ID': user_id
        }

        # Using limit as a query parameter instead of header
        params = {
            'limit': 100
        }

        response = requests.get(
            f"{base_url}/r/company/companies",
            headers=headers,
            params=params,  # Add query parameters
            timeout=30
        )
        response.raise_for_status()

        data = response.json()
        if 'data' not in data:
            raise ValueError("Response missing 'data' field")

        return data['data']
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch companies: {str(e)}")
        raise
    except ValueError as e:
        logging.error(f"Invalid company data response: {str(e)}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error while fetching companies: {str(e)}")
        raise

def get_company_stats(base_url: str, access_token: str, user_id: str) -> List[Dict]:
    """
    Get vulnerability statistics for all companies

    Args:
        base_url: Base URL for the API
        access_token: Valid access token
        user_id: User ID for authentication

    Returns:
        List of company statistics dictionaries

    Raises:
        requests.exceptions.RequestException: If API request fails
        ValueError: If response data is invalid
    """
    try:
        headers = {
            'Authorization': f'Bearer {access_token}',
            'X-USER-ID': user_id
        }

        # Using limit as a query parameter instead of header
        params = {
            'limit': 10000
        }

        response = requests.get(
            f"{base_url}/r/company/company_stats",
            headers=headers,
             params=params,  # Add query parameters
            timeout=60
        )
        response.raise_for_status()

        data = response.json()
        if 'data' not in data:
            raise ValueError("Response missing 'data' field")

        return data['data']
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch company stats: {str(e)}")
        raise
    except ValueError as e:
        logging.error(f"Invalid company stats response: {str(e)}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error while fetching company stats: {str(e)}")
        raise

def process_vulnerability_data(companies: List[Dict], stats: List[Dict]) -> pd.DataFrame:
    """
    Process vulnerability data and create a DataFrame

    Args:
        companies: List of company dictionaries
        stats: List of company statistics dictionaries

    Returns:
        DataFrame containing processed vulnerability data

    Raises:
        ValueError: If input data is invalid
    """
    try:
        if not companies or not stats:
            raise ValueError("Empty input data")

        # Create a mapping of company IDs to names
        company_names = {company['id']: company['name'] for company in companies}

        # Define criticality levels and their corresponding problem group IDs
        criticality_mapping = {
            'Critical': ['1'],
            'High': ['2'],
            'Medium': ['3'],
            'Low': ['4']
        }

        # Prepare data for DataFrame
        rows = []
        for stat in stats:
            company_id = stat.get('company_id')
            if not company_id or company_id not in company_names:
                logging.warning(f"Skipping invalid company ID: {company_id}")
                continue

            row = {
                'Company': company_names[company_id],
                'Total_Assets': stat.get('total_assets', 0)
            }

            # Add vulnerability counts for each criticality level
            for criticality, problem_ids in criticality_mapping.items():
                count = sum(stat.get('asset_problem_stats', {}).get(pid, 0) for pid in problem_ids)
                row[f'{criticality}'] = count

            rows.append(row)

        if not rows:
            raise ValueError("No valid data to process")

        return pd.DataFrame(rows)
    except Exception as e:
        logging.error(f"Error processing vulnerability data: {str(e)}")
        raise

def get_last_day_of_quarter(quarter_label: str) -> str:
    """
    Get the last day of the quarter as a string in YYYY-MM-DD format.

    Args:
        quarter_label: The quarter label (e.g., "Q3 2023").

    Returns:
        The last day of the quarter in YYYY-MM-DD format.
    """
    year = int(quarter_label.split(" ")[1])
    quarter = int(quarter_label.split(" ")[0][1:])

    if quarter == 1:
        last_day = datetime(year, 3, 31)
    elif quarter == 2:
        last_day = datetime(year, 6, 30)
    elif quarter == 3:
        last_day = datetime(year, 9, 30)
    else:  # quarter == 4
        last_day = datetime(year, 12, 31)

    return last_day.strftime("%Y-%m-%d")

def upload_to_airtable(df: pd.DataFrame):
    """
    Upload DataFrame data to Airtable, one record per company per quarter,
    per criticality level.

    Args:
        df: DataFrame containing vulnerability data.
    """
    airtable_url = f"https://api.airtable.com/v0/{AIRTABLE_BASE_ID}/{AIRTABLE_TABLE_NAME}"
    headers = {
        "Authorization": f"Bearer {AIRTABLE_API_KEY}",
        "Content-Type": "application/json"
    }

    current_quarter_label = f"Q{((datetime.now().month - 1) // 3) + 1} {datetime.now().year}"
    last_day_of_quarter = get_last_day_of_quarter(current_quarter_label)

    for index, row in df.iterrows():
        # Iterate through each criticality level and create a separate record
        for status in ["Critical", "High", "Medium", "Low"]:
            fields = {
                "Company Name": row["Company"],
                "Count": row[status],  # Use the count for the specific criticality
                "Status": status,      # Set the criticality level
                "Quarter": last_day_of_quarter
            }

            record_data = {"fields": fields}

            try:
                response = requests.post(airtable_url, headers=headers, json=record_data)
                response.raise_for_status()
                logging.info(f"Successfully uploaded data for {row['Company']}, Status: {status} to Airtable")
                print(f"Successfully uploaded data for {row['Company']}, Status: {status} to Airtable")

            except requests.exceptions.HTTPError as e:
                logging.error(f"HTTP error while uploading data for {row['Company']}, Status: {status}: {e.response.status_code} - {e.response.text}")
                print(f"HTTP error while uploading data for {row['Company']}, Status: {status}: {e.response.status_code} - {e.response.text}")

            except requests.exceptions.RequestException as e:
                logging.error(f"Failed to upload data to Airtable for {row['Company']}, Status: {status}: {str(e)}")
                print(f"Failed to upload data to Airtable for {row['Company']}, Status: {status}: {str(e)}")

            finally:
                time.sleep(SECONDS_PER_REQUEST)
                print(f"Waiting {SECONDS_PER_REQUEST} seconds due to rate limiting...")

def main():
    
    try:
        # Get authentication token
        logging.info("Getting authentication token...")
        access_token, user_id = get_auth_token(BASE_URL, CLIENT_AUTH_TOKEN)

        # Get companies and stats
        logging.info("Fetching company data...")
        companies = get_companies(BASE_URL, access_token, user_id)

        logging.info("Fetching company statistics...")
        stats = get_company_stats(BASE_URL, access_token, user_id)

        # Process data
        logging.info("Processing vulnerability data...")
        df = process_vulnerability_data(companies, stats)

        # Upload to Airtable
        logging.info("Uploading data to Airtable...")
        upload_to_airtable(df)

    except requests.exceptions.RequestException as e:
        logging.error(f"API request failed: {str(e)}")
        raise SystemExit(1)
    except ValueError as e:
        logging.error(f"Data validation error: {str(e)}")
        raise SystemExit(1)
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        raise SystemExit(1)

if __name__ == "__main__":
    main()
