import requests
import base64
import csv
import json
from datetime import datetime
import difflib
from collections import defaultdict
import logging
import sys

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# API Configuration
BASE_URL = "https://<Your Pod Here>.myconnectsecure.com"
TENANT = ""
CLIENT_ID = ""
CLIENT_SECRET = ""

def get_auth_token():
    auth_string = f""
    encoded_auth = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')
    
    headers = {
        "Client-Auth-Token": encoded_auth
    }
    
    response = requests.post(f"{BASE_URL}/w/authorize", headers=headers)
    if response.status_code == 200:
        return response.json()['access_token'], response.json()['user_id']
    else:
        logging.error(f"Authentication failed. Status code: {response.status_code}")
        logging.error(f"Response content: {response.text}")
        raise Exception("Authentication failed")

def get_companies(token, user_id, limit=100):
    headers = {
        "Authorization": f"Bearer {token}",
        "X-USER-ID": user_id
    }
    
    params = {
        "limit": limit
    }
    
    try:
        response = requests.get(f"{BASE_URL}/r/company/companies", headers=headers, params=params)
        response.raise_for_status()  # This will raise an HTTPError for bad responses
        return response.json()['data']
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch companies. Error: {str(e)}")
        logging.error(f"Response status code: {response.status_code}")
        logging.error(f"Response content: {response.text}")
        raise Exception("Failed to fetch companies")

def get_all_remediations(token, user_id):
    headers = {
        "Authorization": f"Bearer {token}",
        "X-USER-ID": user_id
    }
    
    params = {
        "limit": 100000
    }
    
    try:
        response = requests.get(f"{BASE_URL}/r/report_queries/get_remediation", headers=headers, params=params)
        response.raise_for_status()
        return response.json()['data']
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch remediations. Error: {str(e)}")
        logging.error(f"Response status code: {response.status_code}")
        logging.error(f"Response content: {response.text}")
        raise Exception("Failed to fetch remediations")

def filter_high_epss(remediations):
    return [r for r in remediations if r.get('epss_vuls', 0) >= 0.3]

def filter_network_vuls(remediations):
    return [r for r in remediations if
            r.get('severity') in ['Critical', 'High'] and
            (r.get('fix') == 'Check remediation script' or
             r.get('fix') == 'Remediate port vulnerabilities by closing ports, but ensure awareness of any dependencies before taking action.')]

def safe_join(value):
    if isinstance(value, list):
        return ', '.join(str(item) for item in value)
    elif value is None:
        return ''
    else:
        return str(value)

def write_csv_report(filename, remediations, fieldnames):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for remediation in remediations:
            # Safely convert potentially problematic fields
            remediation['fix_script'] = safe_join(remediation.get('fix_script'))
            remediation['install_source'] = safe_join(remediation.get('install_source'))
            
            # Write the row, only including fields that are in fieldnames
            writer.writerow({k: v for k, v in remediation.items() if k in fieldnames})
    
    logging.info(f"Report written to {filename}")

def find_closest_match(name, name_list):
    return difflib.get_close_matches(name, name_list, n=1, cutoff=0.6)

def main():
    print("Script started.")
    try:
        print("Attempting to authenticate...")
        token, user_id = get_auth_token()
        print("Authentication successful.")
        logging.info("Authentication successful")
        
        print("Fetching companies...")
        # Fetch companies with pagination
        all_companies = []
        offset = 0
        while True:
            print(f"Fetching companies batch. Offset: {offset}")
            companies_batch = get_companies(token, user_id, limit=100)
            all_companies.extend(companies_batch)
            if len(companies_batch) < 100:
                break
            offset += 100
        
        print(f"Total companies fetched: {len(all_companies)}")
        logging.info(f"Total companies fetched: {len(all_companies)}")
        
        print("Fetching all remediations...")
        all_remediations = get_all_remediations(token, user_id)
        print(f"Total remediations fetched: {len(all_remediations)}")
        logging.info(f"Total remediations fetched: {len(all_remediations)}")
        
        # Create a set of unique company names from the remediations
        remediation_company_names = set(r.get('company_name') for r in all_remediations if r.get('company_name'))
        
        # Create a dictionary to store company name variations
        company_name_map = {company['name'].lower(): company['name'] for company in all_companies}
        
        current_month = datetime.now().strftime("%B%Y")
        
        fieldnames = [
            'solution_id', 'os_type', 'fix', 'url', 'product', 'remediation_action',
            'epss_vuls', 'severity', 'critical_vuls_count', 'high_vuls_count',
            'medium_vuls_count', 'low_vuls_count', 'total_vuls_count',
            'first_vul_discovered', 'last_vul_discovered', 'install_source',
            'asset_id', 'company_id', 'agent_id', 'host_name', 'ip_addresses',
            'created', 'company_name', 'fix_script'
        ]
        
        for company in all_companies:
            company_name = company['name']
            company_id = company['id']
            
            print(f"\nProcessing company: {company_name}")
            logging.info(f"\nProcessing company: {company_name}")
            
            try:
                # Filter remediations for this specific company, considering variations
                company_remediations = [
                    r for r in all_remediations 
                    if r.get('company_name') and (
                        r.get('company_name').lower() == company_name.lower() or
                        company_name_map.get(r.get('company_name').lower()) == company_name
                    )
                ]
                
                if not company_remediations:
                    # Try to find a close match
                    closest_match = find_closest_match(company_name, remediation_company_names)
                    if closest_match:
                        print(f"No exact match found. Closest match: {closest_match[0]}")
                        logging.info(f"No exact match found. Closest match: {closest_match[0]}")
                        company_remediations = [
                            r for r in all_remediations 
                            if r.get('company_name') == closest_match[0]
                        ]
                
                if company_remediations:
                    # High EPSS report
                    high_epss_remediations = filter_high_epss(company_remediations)
                    if high_epss_remediations:
                        write_csv_report(f"{company_name}_epss_report_{current_month}.csv", high_epss_remediations, fieldnames)
                    
                   # Network vulnerabilities report (with combined conditions)
                    network_vuls = filter_network_vuls(company_remediations)
                    if network_vuls:
                        write_csv_report(f"{company_name}_network_vuls_{current_month}.csv", network_vuls, fieldnames)

                    
                    print(f"Reports generated for {company_name}")
                    print(f"  High EPSS vulnerabilities: {len(high_epss_remediations)}")
                    print(f"  Network vulnerabilities (Critical/High, Check remediation script or Port remediation): {len(network_vuls)}")

                    logging.info(f"Reports generated for {company_name}")
                    logging.info(f"  High EPSS vulnerabilities: {len(high_epss_remediations)}")
                    logging.info(f"  Network vulnerabilities (Critical/High, Check remediation script or Port remediation): {len(network_vuls)}")

                else:
                    print(f"No vulnerabilities or remediations found for {company_name}")
                    logging.info(f"No vulnerabilities or remediations found for {company_name}")
            
            except Exception as e:
                print(f"Error processing {company_name}: {str(e)}")
                logging.error(f"Error processing {company_name}: {str(e)}")
        
        print("\nReport generation complete.")
        logging.info("\nReport generation complete.")
    
    except Exception as e:
        print(f"An error occurred in the main function: {str(e)}")
        logging.error(f"An error occurred in the main function: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Unhandled exception: {str(e)}")
        logging.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)
